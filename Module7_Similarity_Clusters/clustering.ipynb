{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "clustering.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O15E-LZehO2q"
      },
      "source": [
        "# Similarity, Neighbors, and Clustering\n",
        "\n",
        "\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzRbpAGNhO2t",
        "outputId": "a9be938f-2faa-458f-d507-4bdb96832973",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Import the libraries we will be using\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "from scipy.spatial import distance\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from collections import defaultdict\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "! git clone https://github.com/yizuc/datamining.git\n",
        "\n",
        "from datamining.ds_utils.decision_surface import *\n",
        "\n",
        "sns.set(font_scale=1.5)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'datamining'...\n",
            "remote: Enumerating objects: 571, done.\u001b[K\n",
            "remote: Counting objects: 100% (571/571), done.\u001b[K\n",
            "remote: Compressing objects: 100% (330/330), done.\u001b[K\n",
            "remote: Total 571 (delta 246), reused 527 (delta 221), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (571/571), 165.92 MiB | 28.07 MiB/s, done.\n",
            "Resolving deltas: 100% (246/246), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqCbGtRmhO2u"
      },
      "source": [
        "## Lights, camera, action!\n",
        "\n",
        "You've been hired by Netflakes as a business analytics professional. Netflakes' primary business is its subscription-based streaming service which offers online streaming of a library of films and television programs, including those produced in-house (sounds familiar?). A major competitive advantage of Netflakes is its wide variety of films. You've been hired to understand what kind of movies people like. Hopefully, your analysis will inspire ideas for new movies! \n",
        "\n",
        "Let's start by taking a look at the data we have available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbdxZ066hO2v"
      },
      "source": [
        "# Import the ratings dataset\n",
        "df_ratings = pd.read_csv('/content/datamining/Module7_Similarity_Clusters/data/ratings.csv')\n",
        "df_ratings.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEpxwhnkhO2w"
      },
      "source": [
        "# Import movies dataset\n",
        "df_movies = pd.read_csv('/content/datamining/Module7_Similarity_Clusters/data/movies.csv')\n",
        "df_movies.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyjSv-SThO2w"
      },
      "source": [
        "print('The dataset contains: ', len(df_ratings), ' ratings of ', len(df_movies), ' movies.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqmtGqSkhO2x"
      },
      "source": [
        "# Replace name of (no genres listed)\n",
        "df_movies.genres = df_movies.genres.replace(\"(no genres listed)\", \"NA\")\n",
        "# Counts per genre (films may appear in more than one genre)\n",
        "pd.Series(df_movies.genres.str.cat(sep=\"|\").split(\"|\")).value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoKEXH61hO2x"
      },
      "source": [
        "## Feature representation\n",
        "How can we turn these ratings into a useful representation of user tastes (i.e., features)?\n",
        "\n",
        "\n",
        "### Avg. Ratings\n",
        "One way is to define each feature vector as movie ratings or average genre rating. Let's start with genre."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6BpP_qAhO2y"
      },
      "source": [
        "def get_genre_ratings(ratings, movies, genres, mean=True):\n",
        "    all_genre_df = pd.DataFrame()\n",
        "    for genre in genres:        \n",
        "        genre_movies = movies[movies['genres'].str.contains(genre)]\n",
        "        relevant_ratings = ratings[ratings['movieId'].isin(genre_movies['movieId'])]\n",
        "        if mean is True:\n",
        "            single_genre_df = relevant_ratings.groupby(['userId'])['rating'].mean().round(2)\n",
        "        else:\n",
        "            single_genre_df = relevant_ratings.groupby(['userId'])['rating'].count()\n",
        "        all_genre_df = pd.concat([all_genre_df, single_genre_df], axis=1)\n",
        "    all_genre_df.columns = genres\n",
        "    return all_genre_df\n",
        "\n",
        "genres = ['Action', 'Comedy']\n",
        "users_by_avg_genre_ratings = get_genre_ratings(df_ratings, df_movies, genres)\n",
        "sns.lmplot(genres[0], genres[1], data=users_by_avg_genre_ratings, fit_reg=False, height=8)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8JCYxgshO2z"
      },
      "source": [
        "All genre ratings seem to be positively correlated. What's going on?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xihVYE9hO2z"
      },
      "source": [
        "### Number of ratings\n",
        "Let's try as features the total numbers of movies of each genre that users have rated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa7cyKHdhO2z"
      },
      "source": [
        "genres = ['Action', 'Comedy']\n",
        "users_by_total_genre_ratings = get_genre_ratings(df_ratings, df_movies, genres, False)\n",
        "sns.lmplot(genres[0], genres[1], data=users_by_total_genre_ratings, fit_reg=False, height=8)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skIUaYpIhO20"
      },
      "source": [
        "Still everything seems correlated, why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUowrAAxhO20"
      },
      "source": [
        "### Percentage of ratings\n",
        "What if we instead define the feature vector as the percentage of ratings belonging to each genre?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOk3ZNQQhO20"
      },
      "source": [
        "def get_genre_shares(ratings, movies):\n",
        "    all_genres = np.unique(df_movies.genres.str.cat(sep=\"|\").split(\"|\"))\n",
        "    all_genre_df = pd.DataFrame()\n",
        "    for genre in all_genres:\n",
        "        genre_movies = movies[movies['genres'].str.contains(genre)]\n",
        "        relevant_ratings = ratings[ratings['movieId'].isin(genre_movies['movieId'])]\n",
        "        single_genre_df = relevant_ratings.loc[:, ['userId', 'rating']].groupby(['userId'])['rating'].count()\n",
        "        all_genre_df = pd.concat([all_genre_df, single_genre_df], axis=1)\n",
        "    # Get shares\n",
        "    all_genre_df.columns = all_genres\n",
        "    all_genre_df.fillna(0, inplace=True)\n",
        "    all_genre_df = all_genre_df.div(ratings.groupby('userId').rating.count(), axis=0)\n",
        "    return all_genre_df\n",
        "\n",
        "genres = ['Action', 'Comedy']\n",
        "users_by_rating_per = get_genre_shares(df_ratings, df_movies)\n",
        "sns.lmplot(genres[0], genres[1], data=users_by_rating_per, fit_reg=False, height=8)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vnq-t8JnhO21"
      },
      "source": [
        "Now we see different correlations. Do you still see any issues with this representation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9-PWQMnhO21"
      },
      "source": [
        "## Similarity measures\n",
        "\n",
        "Once we have objects described as data, we can compute the similarity between different objects. Each of the users is now described by their tastes for different genres. Let's keep the share of ratings representation for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJ_gCgixhO21"
      },
      "source": [
        "users_by_rating_per.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy4SjhFehO22"
      },
      "source": [
        " So ... how can we tell if users have similar tastes? Generally, how can we compute similarity between users?  We've reduced this question to: how can we compute similarity between objects described as feature vectors.\n",
        "\n",
        "There are many similarity measures.  Similarity is often cast as \"closeness\" in some space, as computed by a distance measure.  Often in data science, the terms similarity and distance are used interchangeably (a little strangely to the uninitiated). \n",
        "\n",
        "We'll use the library scipy.spatial.distance available [here](http://docs.scipy.org/doc/scipy/reference/spatial.distance.html)\n",
        "\n",
        "This library has functions to compute the distance between two numeric vectors. In particular, **pdist(X[, metric, p, w, V, VI])**\tcomputes pairwise distances between the observations in n-dimensional space. _Metric parameter: The distance function can be ‘braycurtis’, ‘canberra’, ‘chebyshev’, ‘cityblock’, ‘correlation’, ‘cosine’, ‘dice’, ‘euclidean’, ‘hamming’, ‘jaccard’, ‘kulsinski’, ‘mahalanobis’, ‘matching’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’, ‘yule’._\n",
        "\n",
        "Here is a function that will compute the distance using as many metrics as you want:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ99hh2JhO22"
      },
      "source": [
        "def user_distance(users, userId, distance_measures, n):\n",
        "    # We want a data frame to store the output\n",
        "    # distance_measures is a list of the distance measures you want to compute (see below)\n",
        "    # n is how many \"most similar\" to report\n",
        "    distances = pd.DataFrame()\n",
        "    # Find the location of the whiskey we are looking for\n",
        "    user_location = np.where(users.index == userId)[0][0]\n",
        "    # Go through all distance measures we care about\n",
        "    for distance_measure in distance_measures:\n",
        "        # Find all pairwise distances\n",
        "        current_distances = distance.squareform(distance.pdist(users, distance_measure))\n",
        "        # Get the closest n elements for the user we care about\n",
        "        most_similar = np.argsort(current_distances[:, user_location])[0:n]\n",
        "        # Append results (a new column to the dataframe with the name of the measure)\n",
        "        distances[distance_measure] = list(zip(users.index[most_similar], current_distances[most_similar, user_location]))\n",
        "    return distances"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKrJzxRbhO22"
      },
      "source": [
        "We can use the function `user_distance` to find the distance value of each user with respect to others. We'll start using Euclidean distance as our metric:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5UHayxMhO23"
      },
      "source": [
        "user_distance(users_by_rating_per, 1, ['euclidean'], 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nse3a1UhO23"
      },
      "source": [
        "Now, let's use more meatrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBafQ5yXhO23"
      },
      "source": [
        "user_distance(users_by_rating_per, 1, ['euclidean', 'cityblock', 'cosine', 'correlation'], 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJ1Ew4buhO23"
      },
      "source": [
        "users_by_rating_per.loc[[1, 63, 328, 212]].sort_values(by=1, axis=1, ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyX_hhfmhO23"
      },
      "source": [
        "## Clustering Methods\n",
        "\n",
        "Similarity has many uses in data science.  One of the most commonly discussed is clustering: Can we find groups of users that are similar?\n",
        "\n",
        "### Hierarchical Clustering\n",
        "\n",
        "There are different ways to find similar groups.  One very common method is Hierarchical Clustering.\n",
        "\n",
        "First let's look at a simple example to illustrate.  Given a set of records (A-F) with two features, we can visualize them on a 2 dimensional surface.  Clustering proceeds as follows.  First consider each point to be its own cluster.  Then, iteratively, group together the closest two clusters.  In the figure, circles were drawn in order of grouping.  The second diagram is a visualization of the hierarchy of groupings, called a \"dendrogram.\"  You can clip it at any point, vertically, and get \"the best\" clustering for a certain number of groups.\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/yizuc/datamining/blob/master/Module7_Similarity_Clusters/images/cutting.png?raw=1\" height=40% width=40%>\n",
        "\n",
        "Let's examine the dendrogram(s) for our data, we'll be using the library: **scipy.cluster.hierarchy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sZKseHXhO24"
      },
      "source": [
        "sample = users_by_rating_per.sample(80, random_state=42)\n",
        "\n",
        "# This function gets pairwise distances between observations in n-dimensional space (e.g., cosine, euclidean).\n",
        "dists = distance.pdist(sample, metric=\"cosine\")\n",
        "\n",
        "# This scipy's function performs hierarchical/agglomerative clustering on the condensed distance matrix y.\n",
        "# Method could be 'average' distance from points in cluster v to points in cluster w or the 'single' shortest distance\n",
        "links = linkage(dists, method='single')\n",
        "\n",
        "# Now we want to plot those 'links' using \"dendrogram\" function\n",
        "plt.rcParams['figure.figsize'] = 32, 16\n",
        "\n",
        "den = dendrogram(links)\n",
        "\n",
        "plt.xlabel('Samples',fontsize=30)\n",
        "plt.ylabel('Distance',fontsize=30)\n",
        "plt.xticks(rotation=90,fontsize=20)\n",
        "plt.yticks(fontsize=20)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywhxJnrNhO24"
      },
      "source": [
        "It is common to cut dendrograms at a particular height and to then use the resulting clusters. However, watch out for two things when doing hierarchical clusters: (1) it does not scale well with large data sets and (2) it's very sensitive to outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68FyAmXNhO24"
      },
      "source": [
        "### KMeans\n",
        "\n",
        "Another method for finding clusters is to use the KMeans algorithm to find a set of $k$ clusters. Here, unlike in hierarchical clustering, we define the number of clusters in advance. We'll use the library **sklearn.cluster**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xP-hPZachO24"
      },
      "source": [
        "k_clusters = 2\n",
        "kmeans = KMeans(n_clusters=k_clusters, random_state=42)\n",
        "genres = ['Thriller', 'Mystery']\n",
        "kmeans.fit(users_by_rating_per[genres])\n",
        "users_by_rating_per['cluster'] = kmeans.predict(users_by_rating_per[genres])\n",
        "sns.lmplot(genres[0], genres[1], data=users_by_rating_per, hue='cluster', fit_reg=False, height=8)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbznWJgvhO24"
      },
      "source": [
        "We are using euclidean distance to find these clusters. Do you have any concerns about this?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOCLEbyThO24"
      },
      "source": [
        "df_normalized_genre = (users_by_rating_per - users_by_rating_per.mean())/users_by_rating_per.std()\n",
        "k_clusters = 2\n",
        "kmeans = KMeans(n_clusters=k_clusters, random_state=42)\n",
        "genres = ['Thriller', 'Mystery']\n",
        "kmeans.fit(df_normalized_genre[genres])\n",
        "df_normalized_genre['cluster'] = kmeans.predict(df_normalized_genre[genres])\n",
        "sns.lmplot(genres[0], genres[1], data=df_normalized_genre, hue='cluster', fit_reg=False, height=8)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuP-K1pshO25"
      },
      "source": [
        "KMeans is very sensitive to scale and will tend to cluster according to the features with greater variance.\n",
        "\n",
        "What happens if we use all the genres to cluster users?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acbJu7O6hO25"
      },
      "source": [
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "all_genres = df_normalized_genre.columns[df_normalized_genre.columns != 'cluster']\n",
        "genres = ['Action', 'Comedy']\n",
        "df_normalized_genre['cluster'] = kmeans.fit_predict(df_normalized_genre[all_genres]).astype(int)\n",
        "sns.lmplot(genres[0], genres[1], data=df_normalized_genre, hue='cluster', fit_reg=False, height=8)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aAzLSOwhO25"
      },
      "source": [
        "Then, how can we describe or name each cluster?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E4iWJqehO25"
      },
      "source": [
        "centroids = df_normalized_genre.groupby('cluster').mean()\n",
        "centroids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKPO0QwNhO26"
      },
      "source": [
        "data = defaultdict(list)\n",
        "top = 3\n",
        "\n",
        "clusters = sorted(df_normalized_genre.cluster.unique())\n",
        "for cluster in clusters:\n",
        "    tastes = centroids.loc[cluster].sort_values()\n",
        "    for i, t in enumerate(tastes[:top].index):\n",
        "        data[\"Dislike {0}\".format(i + 1)].append(\"{0}: {1}\".format(t, tastes[t].round(2)))\n",
        "    for i, t in enumerate(tastes[-top:][::-1].index):\n",
        "        data[\"Like {0}\".format(i + 1)].append(\"{0}: {1}\".format(t, tastes[t].round(2)))\n",
        "    counts = df_normalized_genre[df_normalized_genre.cluster == cluster].shape[0]\n",
        "    data[\"count\"].append(counts)\n",
        "    \n",
        "cols = ['count'] + [\"Like {0}\".format(i+1) for i in range(top)] + [\"Dislike {0}\".format(i+1) for i in range(top)]\n",
        "cluster_info = pd.DataFrame(data)[cols].transpose()\n",
        "cluster_info.columns = [\"cluster {0}\".format(i) for i in clusters]\n",
        "cluster_info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fujTfU_ZhO26"
      },
      "source": [
        "We could think of these as the \"Unexpected Action\", \"Happy Films\", \"Cartoon Adventure\", and \"Sinister Drama\" clusters. Perhaps we could use this information to develop four new films that would cover a wide variety of tastes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMOkWp2rhO26"
      },
      "source": [
        "## Movie-level Clustering\n",
        "Now that we've covered some ground regarding how Kmeans clusters users based on their genre tastes, let's take a bigger bite and look at how users rated individual movies. To do that, we'll shape the dataset in the form of userId vs user rating for each movie. For example, let's look at a subset of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gKZOwIehO26"
      },
      "source": [
        "# Merge the two tables then pivot so we have Users X Movies dataframe\n",
        "ratings_title = pd.merge(df_ratings, df_movies[['movieId', 'title']], on='movieId')\n",
        "df_user_movie_ratings = pd.pivot_table(ratings_title, index='userId', columns= 'title', values='rating')\n",
        "\n",
        "print('dataset dimensions: ', df_user_movie_ratings.shape, '\\n\\nSubset example:')\n",
        "df_user_movie_ratings.iloc[10:20, :10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HE1LQhnhhO26"
      },
      "source": [
        "Most users have not rated and watched most movies. The dominance of NaN values can be an important issue. Can you tell why?\n",
        "\n",
        "Same as with text, datasets like this are called \"sparse\" because only a small number of cells have values. To get around this, let's sort by the most rated movies, and the users who have rated the most number of movies. That will present a more 'dense' region when we peak at the top of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxvGZvXzhO26"
      },
      "source": [
        "n_movies = 30\n",
        "n_users = 20\n",
        "top_movies = df_user_movie_ratings.count().sort_values()[-n_movies:].index[::-1]\n",
        "top_users = df_user_movie_ratings.count(axis=1).sort_values()[-n_users:].index[::-1]\n",
        "top_mat = df_user_movie_ratings.loc[top_users, top_movies]\n",
        "plt.figure(figsize = (15,8))\n",
        "shorter_titles = [c[:30] for c in top_mat.columns]\n",
        "g = sns.heatmap(top_mat, cmap=plt.cm.coolwarm_r, xticklabels=shorter_titles)\n",
        "g.xaxis.set_ticks_position('top')\n",
        "plt.setp(g.get_xticklabels(), rotation=90)\n",
        "plt.xlabel('')\n",
        "g.set_facecolor('black')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SEBqhNHhO26"
      },
      "source": [
        "Each column is a movie. Each row is a user. The color of the cell is how the user rated that movie based on the scale on the right of the graph.\n",
        "\n",
        "Notice how some cells are black? This means the respective user did not rate that movie. This is an issue you'll come across when clustering in real life. Unlike the clean example we started with, real-world datasets can often be sparse and not have a value in each cell of the dataset. This makes it less straightforward to cluster users directly by their movie ratings as k-means generally does not like missing values. Can you think what to do about it?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ac3V-QShO27"
      },
      "source": [
        "# We will keep the top 1000 movies to cluster people\n",
        "n_movies = 1000\n",
        "top_movieId = ratings_title.movieId.value_counts()[:1000].index\n",
        "ratings_top_movies = ratings_title[np.in1d(ratings_title.movieId, top_movieId)]\n",
        "# Create a sparse version of the Users X Movies dataframe. This will impute a value of 0 for missing data.\n",
        "titles_c = np.array(sorted(ratings_top_movies.title.unique()))\n",
        "users_c = np.array(sorted(ratings_top_movies.userId.unique()))\n",
        "row = pd.Categorical(ratings_top_movies.userId, categories=users_c, ordered=True).codes\n",
        "col = pd.Categorical(ratings_top_movies.title, categories=titles_c, ordered=True).codes\n",
        "sparse_ratings = csr_matrix((ratings_top_movies.rating, (row, col)), shape=(users_c.size, titles_c.size))\n",
        "sparse_ratings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFDmswgZhO27"
      },
      "source": [
        "Let's cluster by movies and take a look at the results! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzKBAOXhhO27"
      },
      "source": [
        "k_clusters = 20\n",
        "kmeans = KMeans(n_clusters=k_clusters, random_state=42)\n",
        "clusters = kmeans.fit_predict(sparse_ratings).astype(int)\n",
        "\n",
        "data = defaultdict(list)\n",
        "top = 5\n",
        "for cluster in range(k_clusters):\n",
        "    top_ixs = np.asarray(sparse_ratings[clusters == cluster, :].mean(axis=0).argsort()[0, -top:]).reshape(-1)\n",
        "    for i, ix in enumerate(top_ixs):\n",
        "        data[\"Top {0} Movie\".format(i + 1)].append(titles_c[ix])\n",
        "    counts = clusters[clusters == cluster].size\n",
        "    data[\"count\"].append(counts)\n",
        "    \n",
        "cols = ['count'] + [\"Top {0} Movie\".format(i+1) for i in range(top)]\n",
        "cluster_info = pd.DataFrame(data)[cols].transpose()\n",
        "cluster_info.columns = [\"cluster {0}\".format(i) for i in range(k_clusters)]\n",
        "cluster_info.iloc[:, cluster_info.loc['count'].argsort()[::-1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOpSx1G5hO27"
      },
      "source": [
        "We can see some clear trends in the clusters. For example: \n",
        "* Cluster 18 likes crime movies\n",
        "* Clusters 4 and 1 like Star Wars and Terminator.\n",
        "* Cluster 10 likes Lord of the Rings.\n",
        "\n",
        "Can you think of other ways to represent users and cluster them?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41JP8xiMhO27"
      },
      "source": [
        "## Nearest Neighbors: Recommending Movies\n",
        "\n",
        "Imagine that we want to make movie recommendations to a user. Can we use similarity to do this? As an example, let's find the nearest neighbors to userId 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXU2E8X9hO27"
      },
      "source": [
        "# Find k=10 nearest neighbors\n",
        "k = 10\n",
        "nn_model = NearestNeighbors(n_neighbors=k+1).fit(sparse_ratings)\n",
        "distances, indices = nn_model.kneighbors(sparse_ratings[0,:])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFHpouZMhO27"
      },
      "source": [
        "Now, let's find the movies that were liked the most by taking the average rating. Do you think this is a good idea? Why yes or why not?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzxpbFIphO28"
      },
      "source": [
        "# Get average ratings for each movie according to nearest neighbors\n",
        "avg_ratings = np.asarray(sparse_ratings[indices[0, 1:], :].mean(axis=0)).reshape(-1)\n",
        "# Get top recommendations\n",
        "k_rec = 10\n",
        "top_recommendations = titles_c[avg_ratings.argsort()[::-1][:k_rec]]\n",
        "# Check  whether the user has already seen these movies.\n",
        "for title in top_recommendations:\n",
        "    user_rating = df_user_movie_ratings.loc[1, title]\n",
        "    if  pd.isnull(user_rating):\n",
        "        print(\"RECOMMEND:\", title)\n",
        "    else:\n",
        "        print(\"USER GAVE RATING\", user_rating, \"TO\",  title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jwaege3LhO28"
      },
      "source": [
        "The nearest neighbors like Terminator 2. What if we wanted to predict how much the user would like this recommendation? How could we do that?\n",
        "\n",
        "That's a supervised learning task. If we have a target variable to estimate/predict and labels for a training set,  we can do prediction directly using similarity.\n",
        "\n",
        "In this case, our label would be the rating for the Terminator 2 movie. One way to use similarity to build a predictor<sup>&dagger;</sup> is to use a **Nearest Neighbor algorithm**.  The idea is: to predict the value of the target variable for a data item, first find the most similar (closest) training data items.  The **k-Nearest-Neighbor** or **kNN** algorithm chooses the closest `k` data points.  Then, gather the values of the target variable for them, and then combine them somehow.  So, to classify, one might combine them by having them vote their classes. (How would you combine to compute probability estimates? What about a regression problem?)\n",
        "\n",
        "For now, let's take a look at the ratings of the top recommended movies for each nearest neighbor.\n",
        "\n",
        "<sup>&dagger;</sup>There's an interesting question as to whether we're actually building a *model* here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "nvZp7R2ehO28"
      },
      "source": [
        "top_neighbors_movies = df_user_movie_ratings.iloc[indices[0, 1:]][top_recommendations] \n",
        "top_neighbors_movies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFte9U0ZhO28"
      },
      "source": [
        "Let's take the average rating now (excluding people that did not rate the movie) and compare that to the user ratings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmt9MpXUhO28"
      },
      "source": [
        "df_rec = pd.concat([top_neighbors_movies.mean(), df_user_movie_ratings.loc[1, top_recommendations]], axis=1)\n",
        "df_rec.columns = ['Avg. Rating', 'UserId 1 Ratings']\n",
        "df_rec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPaIDWp3hO28"
      },
      "source": [
        "Seems like Terminator might not be the best recommendation after all! But how come it was listed as the top recommendation? What would you do differently?\n",
        "\n",
        "### Finding similar movies\n",
        "\n",
        "Suppose we recommend 'Aliens' to userId 1, and the user loves the movie. He asks us to recommend a movie similar to that one. How would you do that? HINT: It's pretty much the same thing we already did."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSrRMMMEhO28"
      },
      "source": [
        "movie = \"Aliens (1986)\"\n",
        "movie_ix = np.where(movie == titles_c)[0][0]\n",
        "# Find k=10 nearest movies\n",
        "k = 10\n",
        "nn_model = NearestNeighbors(n_neighbors=k+1).fit(sparse_ratings.T)\n",
        "distances, indices = nn_model.kneighbors(sparse_ratings.T[movie_ix,:])\n",
        "titles_c[indices[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ER9VPrQhhO29"
      },
      "source": [
        "### Prediction via similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "449KxVn0hO29"
      },
      "source": [
        "In a previous example we decided to predict ratings based on the 10 nearest neighbors. But why not just the nearest neighbor? Or why not the 100 nearest neighbors? To illustrate this, let's go back to our earlier example of describing users in terms of the genres they like. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fu3lmqbthO29"
      },
      "source": [
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "all_genres = df_normalized_genre.columns[df_normalized_genre.columns != 'cluster']\n",
        "genres = ['Drama', 'Fantasy']\n",
        "df_normalized_genre['cluster'] = kmeans.fit_predict(df_normalized_genre[all_genres]).astype(int)\n",
        "sns.lmplot(genres[0], genres[1], data=df_normalized_genre, hue='cluster', fit_reg=False, height=8)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkQlRKXJhO29"
      },
      "source": [
        "Suppose we want to find to which cluster each user belongs based only on their taste for fantasy and drama. How would the decision surface look like?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "HI3OmYsshO29"
      },
      "source": [
        "# Let's start by slitting the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = df_normalized_genre[genres]\n",
        "Y = df_normalized_genre['cluster']\n",
        "\n",
        "for i, k in enumerate([100, 10, 1]):\n",
        "    plt.figure(figsize=[13,10])\n",
        "    model = KNeighborsClassifier(n_neighbors=k)\n",
        "    Decision_Surface(X, genres[0], genres[1], Y, model)\n",
        "    plt.title(\"K=\" + str(k))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvgowBFNhO29"
      },
      "source": [
        "Does this look familiar? How should we determine which K is the best one? You guessed it, in the same way we find optimal complexity control parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8oln-FHhO29"
      },
      "source": [
        "for k in [1, 10, 20, 50, 100, 200, 400]:\n",
        "    model = KNeighborsClassifier(????)\n",
        "    auc = cross_val_score(model, X, Y, scoring=\"roc_auc\", cv=10).mean()\n",
        "    print(\"AUC: {0} with K {1}\".format(round(auc*100, 2), k))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
